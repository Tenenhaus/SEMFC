---
title: "R Notebook"
output: html_notebook
---



```{r}

library(lavaan)
source('data/data_generated_reflective.R')



X = X

sem.model <-'
# latent variable definitions
eta1 =~ X11+X12+X13+X14+X15+X16+X17+X18+X19
eta2 =~ X21+X22+X23
eta3 =~ X31+X32+X33
eta4 =~ X41+X42+X43
eta5 =~ X51+X52+X53
eta6 =~ X61+X62+X63

# Regressions
eta5 ~ eta1 + eta2 + eta6
eta6 ~ eta3 + eta4 + eta5

# residual covariances
eta5 ~~ eta6

'








sem.pen.model <-  '
# latent variable definitions
eta1 =~ 1*X11+X12
eta2 =~ 1*X21+X22+X23
eta3 =~ 1*X31+X32+X33
eta4 =~ 1*X41+X42+X43
eta5 =~ 1*X51+X52+X53
eta6 =~ 1*X61+ X62+X63

# penalisation
pen() * eta1 =~ X13 + X14+X15+X16+X17+X18+X19


# Regressions
eta5 ~ eta1 + eta2 + eta6
eta6 ~ eta3 + eta4 + eta5

# residual covariances
eta5 ~~ eta6


'

out <- sem(sem.model, X)
summary(out)
```


```{r}
library(lslx)
```


```{r}
lslx_fa <- lslx$new(model = sem.pen.model, data = X)
lslx_fa$fit(
  penalty_method = "mcp",
  lambda_grid = exp(seq(log(0.001), log(1), length.out = 50)),
  delta_grid = c(1, 1.5, 2, 3, 5, 10, Inf)
)

```
```{r}
lslx_fa$summarize(selector = "bic", interval = FALSE)
```




```{r}
library(regsem)
```



```{r}
# extractMatrices(out)$A
out.reg <- cv_regsem(out, type="lasso", pars_pen = 2:8,n.lambda=23,jump=.05)
head(round(out.reg$parameters,2),5)
head(round(out.reg$fits,2))
plot(out.reg,show.minimum="BIC")
out.reg$final_pars
summary(out.reg)

```














```{r}
library(lessSEM)
```

```{r}

lavaanSyntax <-'
# latent variable definitions
eta1 =~ X11+X12+l0*X13+ l1*X14+l2*X15+l3*X16 + l4*X17
eta2 =~ X21+X22+X23
eta3 =~ X31+X32+X33
eta4 =~ X41+X42+X43
eta5 =~ X51+X52+X53
eta6 =~ X61+X62+X63

# Regressions
eta5 ~ eta1 + eta2 + eta6
eta6 ~ eta3 + eta4 + eta5

# residual covariances
eta5 ~~ eta6

'

lavaanModel <- lavaan::sem(lavaanSyntax,
                           data = X)


lsem <- lasso(
  # pass the fitted lavaan model
  lavaanModel = lavaanModel,
  # names of the regularized parameters:
  regularized = c("l0", "l1", "l2", "l3", "l4"),
  # in case of lasso and adaptive lasso, we can specify the number of lambda
  # values to use. lessSEM will automatically find lambda_max and fit
  # models for nLambda values between 0 and lambda_max. For the other
  # penalty functions, lambdas must be specified explicitly
  nLambdas = 50)



```
```{r}
plot(lsem)
```
```{r}
# coef(lsem)
# coef(lsem, criterion = "AIC")



coef(lsem, criterion = "BIC")
estimates(lsem, criterion = "BIC")

```




```{r}
library(PMA)
```
```{r}
 u <- matrix(c(rnorm(50), rep(0,150)),
 ncol=1)
 v <- matrix(c(rnorm(75),rep(0,225)), ncol=1)
 x <- u%*%t(v)+ matrix(rnorm(200*300),ncol=300)
res <- svd(x,1,1)
```
```{r}
cv.out <- PMD.cv(x, type="standard", sumabss=seq(0.1, 0.6, len=20))
print(cv.out)
plot(cv.out)
```
```{r}
out <- PMD(x, type="standard", sumabs=cv.out$bestsumabs, K=1, v=cv.out$v.init)
print(out)
par(mfrow=c(2,2))
par(mar=c(2,2,2,2))
plot(out$u[,1], main="Est. u")
plot(out$v[,1], main="Est. v")
plot(u, main="True u")
plot(v, main="True v")
plot(res$u[,1], main="svd. u")
plot(res$v[,1], main="svd. v")
plot(u, main="True u")
plot(v, main="True v")
 # And if we want to cont
```
```{r}
out2 <- PMD(x, type="standard", K=2, sumabsu=6, sumabsv=8, v=out$v.init,
cnames=paste("v", sep=" ", 1:ncol(x)), rnames=paste("u", sep=" ", 1:nrow(x)))
print(out2)
```

```{r}
cbind(res$u, u, abs(res$u - u ),out$u, u , abs(out$u - u ))
```
```{r}
set.seed(123)

# Données simulées
X <- matrix(rnorm(100*10), nrow = 100, ncol = 10)
S <- cov(X)   # matrice de covariance (symétrique 10x10)

# --- Cas 1 : Sparse PCA via SPC ---
spc_res <- SPC(S, sumabsv = 3, K = 1)
cat("SPC result (u):\n")
print(round(spc_res$v, 3))   # vecteur sparse

# --- Cas 2 : PMD avec aucune pénalisation sur u (équivaut à SPC) ---
pmd_res <- PMD(S, type="standard",
               sumabsu = sqrt(nrow(S)), # pas de pénalisation sur u
               sumabsv = 3,             # pénalisation sur v
               K = 1)
cat("\nPMD result (u and v):\n")
print(round(pmd_res$u, 3))
print(round(pmd_res$v, 3))

# --- Vérif que u et v sont identiques ---
all.equal(pmd_res$u, pmd_res$v)
```


```{r}
set.seed(123)


# 1) Générer un vecteur u non normalisé
p <- 10
u_orig <- c(2, -1.5, 0, 0, 1, rep(0, p-5))  # non normalisé
cat("u original (non normalisé) :\n")
print(round(u_orig, 3))

# 2) Construire X = u u^T + bruit
X <- u_orig %*% t(u_orig) + 0.1 * matrix(rnorm(p*p), p, p)
X <- (X + t(X))/2  # symétriser






# 3) Appliquer PMD pour retrouver un vecteur normalisé
res <- PMD(X, type="standard",
           sumabsu = 3,  # pas de pénalisation sur u
           sumabsv = 3,        # pénalisation sur v
           K = 1)
u_hat <- res$v          # vecteur normalisé
cat("\nVecteur estimé par PMD (normalisé) :\n")
print(round(u_hat, 3))

# 4) Reconstruire l'échelle pour retrouver u original approximatif
# L'échelle est sqrt(valeur propre principale)
lambda_hat <- as.numeric(t(u_hat) %*% X %*% u_hat)
u_reconstructed <- sqrt(lambda_hat) * u_hat

cat("\nVecteur reconstruit avec échelle :\n")
print(round(u_reconstructed, 3))

# Comparaison avec u original
cat("\nCorrélation avec u original :\n")
print(cor(u_reconstructed, u_orig))
```
```{r}

p <- 10
u_orig <- c(2, -1.5, 0, 0, 1, rep(0, p-5))  # non normalisé
cat("u original (non normalisé) :\n")
print(round(u_orig, 3))

# 2) Construire X = u u^T + bruit
x <- u%*%t(u)+ matrix(rnorm(200*200),ncol=200)
x <- u%*%t(u)
x <- (x + t(x))/2  # symétriser



cv.out <- PMD.cv(x, type="standard", sumabss=seq(0.1, 0.6, len=20))

out <- PMD(x, type="standard", sumabs=cv.out$bestsumabs, K=1, v=cv.out$v.init)

res <- svd(x, 1,1)


```
```{r}
cbind(-round(out$u*sqrt(out$d),2), round(u,2), -round(res$u*sqrt(res$d[[1]]),2))


```
```{r}
X = ((t(Y$LV1) %*% Y$LV2)/299)%*%t(((t(Y$LV1) %*% Y$LV2)/299))
cv.out <- PMD.cv(X, type="standard", sumabss=seq(1/sqrt(3), 1, len=20))

out <- PMD(X, type="standard", sumabs=cv.out$bestsumabs, K=1, v=cv.out$v.init)
out$u

```
